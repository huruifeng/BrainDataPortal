{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-cell metadata and data processing\n",
    "\n",
    "This notebook processes single cell metadata for visualization, including:\n",
    "- Metadata processing and renaming\n",
    "- UMAP embedding processing\n",
    "- Expression data processing\n",
    "- Pseudo-bulk calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:20:44.102719Z",
     "start_time": "2025-07-22T18:20:44.097667Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def is_categorical(arr, unique_threshold = 20):\n",
    "    \"\"\"\n",
    "    Determine if a list of values behaves like a categorical variable.\n",
    "\n",
    "    Parameters:\n",
    "    arr (list): Input list.\n",
    "    unique_threshold (int or float): Max number or ratio of unique values to consider the list categorical.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the list is considered categorical, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(arr) == 0:\n",
    "        return True\n",
    "\n",
    "    # Unique values\n",
    "    unique_values = len(set(arr))\n",
    "    if unique_threshold < 1:\n",
    "        is_few_uniques = unique_values / len(arr) <= unique_threshold\n",
    "    else:\n",
    "        is_few_uniques = unique_values <= unique_threshold\n",
    "\n",
    "    return is_few_uniques\n",
    "\n",
    "def dumps_compact_lists(obj, indent=4):\n",
    "    pretty = json.dumps(obj, indent=indent)\n",
    "\n",
    "    # Match any list that spans multiple lines\n",
    "    def compact_list(match):\n",
    "        # Extract the list content and remove newlines/extra spaces\n",
    "        items = match.group(1).strip().splitlines()\n",
    "        compacted_items = [item.strip().rstrip(',') for item in items if item.strip()]\n",
    "        return '[' + ','.join(compacted_items) + ']'\n",
    "\n",
    "    # Replace multi-line lists with compact single-line ones\n",
    "    compacted = re.sub(r'\\[\\s*((?:.|\\n)*?)\\s*\\]', compact_list, pretty)\n",
    "\n",
    "    return compacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:20:47.115305Z",
     "start_time": "2025-07-22T18:20:46.825187Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters\n",
    "Set up the paths and parameters for the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:20:49.598687Z",
     "start_time": "2025-07-22T18:20:49.590830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path:  example_data/PMDBS_snRNAseq\n",
      "Kept features:  ['nCount_RNA', 'nFeature_RNA', 'sex', 'cell_type', 'phase', 'G2M_score', 'S_score', 'leiden_res_0.10', 'leiden_res_0.20', 'case', 'sample_id']\n",
      "Sample column:  sample_id\n",
      "Cluster column:  cell_type\n",
      "Condition column:  case\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"example_data/snRNA_MTG_10Samples\"\n",
    "kept_features =[ \"nCount_RNA\", \"nFeature_RNA\", \"sex\", \"MajorCellTypes\", \"updrs\", \"Complex_Assignment\", \"mmse\", \"sample_id\", \"case\",]\n",
    "sample_col = \"sample_id\"\n",
    "cluster_col = \"MajorCellTypes\"\n",
    "condition_col = \"case\"\n",
    "\n",
    "print(\"Dataset path: \", dataset_path)\n",
    "print(\"Kept features: \", kept_features)\n",
    "print(\"Sample column: \", sample_col)\n",
    "print(\"Cluster column: \", cluster_col)\n",
    "print(\"Condition column: \", condition_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:20:52.312655Z",
     "start_time": "2025-07-22T18:20:51.824888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking inputs...\n",
      "Loading metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/jjff_3bd7tvblm0pz1wc96tssf65ss/T/ipykernel_44163/1362571716.py:10: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(dataset_path + \"/raw_metadata.csv\", index_col=0, header=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking inputs...\")\n",
    "if sample_col not in kept_features:\n",
    "    kept_features.append(sample_col)\n",
    "if cluster_col not in kept_features:\n",
    "    kept_features.append(cluster_col)\n",
    "if condition_col not in kept_features:\n",
    "    kept_features.append(condition_col)\n",
    "\n",
    "print(\"Loading metadata...\")\n",
    "metadata = pd.read_csv(dataset_path + \"/raw_metadata.csv\", index_col=0, header=0)\n",
    "metadata = metadata.loc[:, kept_features]\n",
    "\n",
    "# Process condition column\n",
    "metadata = metadata.rename(columns={condition_col: \"Condition\"})\n",
    "kept_features.remove(condition_col)\n",
    "kept_features.append(\"Condition\")\n",
    "\n",
    "## if the column data is float, keep 2 digits after the decimal point\n",
    "# Round only float columns to 2 decimal places\n",
    "metadata[metadata.select_dtypes(include=['float']).columns] = metadata.select_dtypes(include=['float']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process cell IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:20:55.687427Z",
     "start_time": "2025-07-22T18:20:53.850886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming cell id...\n"
     ]
    }
   ],
   "source": [
    "# Handle sample_id column\n",
    "if \"sample_id\" != sample_col:\n",
    "    print(\"Renaming sample id...\")\n",
    "    metadata.drop(\"sample_id\", axis=1, inplace=True, errors=\"ignore\")\n",
    "    metadata = metadata.rename(columns={sample_col: \"sample_id\"})\n",
    "    kept_features.remove(sample_col)\n",
    "    kept_features.append(\"sample_id\")\n",
    "\n",
    "# Rename cell IDs\n",
    "print(\"Renaming cell id...\")\n",
    "new_ids = []\n",
    "sample_cell_n = {}\n",
    "for index, row in metadata.iterrows():\n",
    "    sample_id = row[\"sample_id\"]\n",
    "    if sample_id not in sample_cell_n:\n",
    "        sample_cell_n[sample_id] = 0\n",
    "    sample_cell_n[sample_id] += 1\n",
    "    c_id = sample_id + \"_c\" + str(sample_cell_n[sample_id])\n",
    "    new_ids.append(c_id)\n",
    "metadata[\"cs_id\"] = new_ids\n",
    "\n",
    "barcode_to_cid = metadata[\"cs_id\"].to_dict()\n",
    "\n",
    "metadata[\"barcode\"] = metadata.index.tolist()\n",
    "metadata = metadata.set_index(\"cs_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sample id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:20:57.293762Z",
     "start_time": "2025-07-22T18:20:57.183801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save sample list\n",
    "all_samples = metadata[\"sample_id\"].unique().tolist()\n",
    "with open(dataset_path + \"/sample_list.json\", \"w\") as f:\n",
    "    json.dump(sorted(all_samples), f)\n",
    "\n",
    "# Save cell-to-sample mapping\n",
    "cell_to_sample = metadata[\"sample_id\"].to_dict()\n",
    "with open(f\"{dataset_path}/cellspot_to_sample.json\", \"w\") as f:\n",
    "    json.dump(cell_to_sample, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process cell metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:21:00.398566Z",
     "start_time": "2025-07-22T18:20:59.760287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cell metadata...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing cell metadata...\")\n",
    "metadata.loc[:,kept_features].to_csv(dataset_path + \"/cellspot_metadata_original.csv\")\n",
    "\n",
    "# Separate sample-level and cell-level features\n",
    "sample_level_features = []\n",
    "cell_level_features = []\n",
    "sample_groups = metadata.groupby(\"sample_id\")\n",
    "for feature in kept_features:\n",
    "    is_sample_level = all(group[feature].nunique() <= 2 for _, group in sample_groups)\n",
    "    if is_sample_level:\n",
    "        sample_level_features.append(feature)\n",
    "    else:\n",
    "        cell_level_features.append(feature)\n",
    "\n",
    "cell_meta_list = cell_level_features\n",
    "metadata_lite = metadata.loc[:, cell_meta_list]\n",
    "\n",
    "cell_meta_mapping = {}\n",
    "for cell_meta in cell_meta_list:\n",
    "    # Check if the column is categorical\n",
    "    if is_categorical(metadata_lite[cell_meta], unique_threshold=0.2):\n",
    "        # Convert to categorical\n",
    "        cat_series = metadata_lite[cell_meta].astype(\"category\")\n",
    "\n",
    "        cat_counts = cat_series.value_counts().to_dict()\n",
    "\n",
    "        # Replace original column with codes\n",
    "        metadata_lite[cell_meta] = cat_series.cat.codes\n",
    "\n",
    "        # Store mapping, and calculate the number of cells in each category\n",
    "        mapping = {i: [cat, cat_counts[cat]] for i, cat in enumerate(cat_series.cat.categories)} ## with counts\n",
    "        cell_meta_mapping[cell_meta] = mapping\n",
    "\n",
    "# Save mapping to JSON\n",
    "with open(dataset_path + \"/cellspot_meta_mapping.json\", \"w\") as f:\n",
    "    f.write(dumps_compact_lists(cell_meta_mapping, indent=4))\n",
    "\n",
    "metadata_lite.to_csv(dataset_path + \"/cellspot_metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sample metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:21:04.626577Z",
     "start_time": "2025-07-22T18:21:04.593647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample metadata...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing sample metadata...\")\n",
    "sample_meta_list = sample_level_features\n",
    "sample_meta = metadata.loc[:, sample_meta_list]\n",
    "sample_meta = sample_meta.drop_duplicates()\n",
    "sample_meta = sample_meta.set_index(\"sample_id\")\n",
    "sample_meta.fillna(\"\", inplace=True)\n",
    "sample_meta.to_csv(dataset_path + \"/sample_metadata.csv\")\n",
    "\n",
    "with open(dataset_path + \"/meta_list.json\", \"w\") as f:\n",
    "    json.dump(sorted(cell_meta_list + sample_meta_list), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process UMAP Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:21:09.150711Z",
     "start_time": "2025-07-22T18:21:08.787422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding....\n",
      "Renaming embeddings....\n",
      "Sampling umap...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding....\")\n",
    "embeddings_data = pd.read_csv(dataset_path + \"/raw_umap_embeddings.csv\", index_col=0, header=0)\n",
    "\n",
    "embeddings_data[\"UMAP_1\"] = embeddings_data[\"UMAP_1\"].round(2)\n",
    "embeddings_data[\"UMAP_2\"] = embeddings_data[\"UMAP_2\"].round(2)\n",
    "\n",
    "# Rename embeddings using barcode to cell ID mapping\n",
    "print(\"Renaming embeddings....\")\n",
    "embeddings_data = embeddings_data.reset_index()\n",
    "embeddings_data[\"index\"] = embeddings_data[\"index\"].map(barcode_to_cid)\n",
    "embeddings_data = embeddings_data.set_index(\"index\")\n",
    "embeddings_data.to_csv(dataset_path + \"/umap_embeddings.csv\", index_label=\"cs_id\")\n",
    "\n",
    "## sampling umap, get 100k cells\n",
    "print(\"Sampling umap...\")\n",
    "n_rows = embeddings_data.shape[0]\n",
    "\n",
    "sample_rows = 100000 if n_rows > 100000 else n_rows\n",
    "embeddings_data_nk = embeddings_data.sample(n=sample_rows, random_state=42)\n",
    "embeddings_data_nk.to_csv(dataset_path + \"/umap_embeddings_100k.csv\", index_label=\"cs_id\")\n",
    "\n",
    "sample_rows = 50000 if n_rows > 50000 else n_rows\n",
    "embeddings_data_nk = embeddings_data.sample(n=sample_rows, random_state=42)\n",
    "embeddings_data_nk.to_csv(dataset_path + \"/umap_embeddings_50k.csv\", index_label=\"cs_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:22:04.254104Z",
     "start_time": "2025-07-22T18:21:14.566013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expression data...\n",
      "Renaming expression....\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading expression data...\")\n",
    "expression_data = pd.read_csv(dataset_path + \"/raw_normalized_counts.csv\", index_col=None, header=0)\n",
    "\n",
    "print(\"Renaming expression....\")\n",
    "expression_data[\"cs_id\"] = expression_data[\"Cell\"].map(barcode_to_cid)\n",
    "expression_data.drop(\"Cell\", axis=1, inplace=True)\n",
    "expression_data[\"Expression\"] = expression_data[\"Expression\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:27:10.577118Z",
     "start_time": "2025-07-22T18:25:02.328817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by gene... be patient...\n",
      "Saving gene jsons...\n",
      "Saving gene... be patient...\n",
      "1000/34018\n",
      "2000/34018\n",
      "3000/34018\n",
      "4000/34018\n",
      "5000/34018\n",
      "6000/34018\n",
      "7000/34018\n",
      "8000/34018\n",
      "9000/34018\n",
      "10000/34018\n",
      "11000/34018\n",
      "12000/34018\n",
      "13000/34018\n",
      "14000/34018\n",
      "15000/34018\n",
      "16000/34018\n",
      "17000/34018\n",
      "18000/34018\n",
      "19000/34018\n",
      "20000/34018\n",
      "21000/34018\n",
      "22000/34018\n",
      "23000/34018\n",
      "24000/34018\n",
      "25000/34018\n",
      "26000/34018\n",
      "27000/34018\n",
      "28000/34018\n",
      "29000/34018\n",
      "30000/34018\n",
      "31000/34018\n",
      "32000/34018\n",
      "33000/34018\n",
      "34000/34018\n"
     ]
    }
   ],
   "source": [
    "# Group data by gene\n",
    "print(\"Grouping by gene... be patient...\")\n",
    "grouped_by_gene = expression_data.groupby(\"Gene\")\n",
    "\n",
    "## Save gene jsons\n",
    "print(\"Saving gene jsons...\")\n",
    "\n",
    "all_genes = grouped_by_gene.groups.keys()\n",
    "all_genes = [gene_i.replace(\"/\", \"_\") for gene_i in list(set(all_genes))]\n",
    "with open(dataset_path + \"/gene_list.json\", \"w\") as f:\n",
    "    json.dump(sorted(all_genes), f)\n",
    "\n",
    "# Create directory for genes\n",
    "os.makedirs(dataset_path + \"/gene_jsons\", exist_ok=True)\n",
    "\n",
    "# Save each gene as a separate JSON file\n",
    "print(\"Saving gene... be patient...\")\n",
    "i = 0\n",
    "total_n = len(all_genes)\n",
    "for gene, df in grouped_by_gene:\n",
    "    try:\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"{i}/{total_n}\")\n",
    "\n",
    "        gene_dict = dict(zip(df[\"cs_id\"], df[\"Expression\"]))\n",
    "\n",
    "        safe_gene_name = gene.replace(\"/\", \"_\")\n",
    "\n",
    "        # Create JSON file\n",
    "        file_name = f\"{dataset_path}/gene_jsons/{safe_gene_name}.json\"\n",
    "        with open(file_name, \"w\") as f:\n",
    "            json.dump(gene_dict, f, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing {gene} !!! Check the error_gene.txt\")\n",
    "        with open(dataset_path + \"/error_gene_json.txt\", \"a\") as f_err:\n",
    "            f_err.write(gene + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate pseudo-bulk expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:27:54.250190Z",
     "start_time": "2025-07-22T18:27:21.229097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating pseudo count...\n",
      "Grouping by gene and sample... be patient...\n",
      "1000/34018\n",
      "2000/34018\n",
      "3000/34018\n",
      "4000/34018\n",
      "5000/34018\n",
      "6000/34018\n",
      "7000/34018\n",
      "8000/34018\n",
      "9000/34018\n",
      "10000/34018\n",
      "11000/34018\n",
      "12000/34018\n",
      "13000/34018\n",
      "14000/34018\n",
      "15000/34018\n",
      "16000/34018\n",
      "17000/34018\n",
      "18000/34018\n",
      "19000/34018\n",
      "20000/34018\n",
      "21000/34018\n",
      "22000/34018\n",
      "23000/34018\n",
      "24000/34018\n",
      "25000/34018\n",
      "26000/34018\n",
      "27000/34018\n",
      "28000/34018\n",
      "29000/34018\n",
      "30000/34018\n",
      "31000/34018\n",
      "32000/34018\n",
      "33000/34018\n",
      "34000/34018\n",
      "Done! Feature/Gene data processed and saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating pseudo count...\")\n",
    "os.makedirs(dataset_path + \"/gene_pseudobulk\", exist_ok=True)\n",
    "\n",
    "expression_data[\"sample_id\"] = expression_data[\"cs_id\"].map(cell_to_sample)\n",
    "\n",
    "print(\"Grouping by gene and sample... be patient...\")\n",
    "# Compute pseudo-bulk counts by summing expression values per (sample, gene)\n",
    "pseudo_bulk = expression_data.groupby([\"sample_id\", \"Gene\"])[\"Expression\"].sum().reset_index()\n",
    "# Rename the expression value column\n",
    "pseudo_bulk.rename(columns={\"Expression\": \"pseudobulk_expr\"}, inplace=True)\n",
    "\n",
    "# Save each gene's data to a separate JSON file\n",
    "i = 0\n",
    "for gene, df_gene in pseudo_bulk.groupby(\"Gene\"):\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i}/{total_n}\")\n",
    "    gene_dict = df_gene.set_index(\"sample_id\")[\"pseudobulk_expr\"].to_dict()\n",
    "    safe_gene_name = gene.replace(\"/\", \"_\")\n",
    "    with open(f\"{dataset_path}/gene_pseudobulk/{safe_gene_name}.json\", \"w\") as f:\n",
    "        json.dump(gene_dict, f, indent=4)\n",
    "\n",
    "print(\"Done! Feature/Gene data processed and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
